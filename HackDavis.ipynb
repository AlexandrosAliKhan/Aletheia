{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HackDavis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "tTASJs9chVEG",
        "outputId": "1523871d-397f-4f1e-a348-55623e49a325"
      },
      "source": [
        "# Sources:\r\n",
        "# https://stackoverflow.com/questions/34714070/error-tokenizing-data-c-error-eof-following-escape-character\r\n",
        "# https://stackoverflow.com/questions/26147180/convert-row-to-column-header-for-pandas-dataframe\r\n",
        "\r\n",
        "\r\n",
        "# https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\r\n",
        "\r\n",
        "# ABOVE LINK IS IMPORTANT\r\n",
        "\r\n",
        "# Now maybe work on API? Unless want to improve model ¯\\_(ツ)_/¯ \r\n",
        "# Testing data set is here https://www.kaggle.com/mrisdal/fake-news\r\n",
        "\r\n",
        "# Unfortunately have to reupload data after runtime refresh\r\n",
        "\r\n",
        "import keras\r\n",
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from textblob import TextBlob\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "import numpy as np\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from numpy import array\r\n",
        "\r\n",
        "true_df = pd.read_csv('True.csv',  header=None)\r\n",
        "true_df.rename(columns=true_df.iloc[0],inplace=True)\r\n",
        "true_df = true_df.drop(true_df.index[0])\r\n",
        "\r\n",
        "\r\n",
        "fake_df = pd.read_csv('Fake.csv',  header=None)\r\n",
        "fake_df.rename(columns=fake_df.iloc[0],inplace=True)\r\n",
        "fake_df= fake_df.drop(fake_df.index[0])\r\n",
        "\r\n",
        "\r\n",
        "Vectorizer = TfidfVectorizer(stop_words=\"english\")\r\n",
        "TrueDF = Vectorizer.fit_transform(true_df[\"title\"].to_list())\r\n",
        "FakeDF = Vectorizer.fit_transform(fake_df[\"title\"].to_list())\r\n",
        "sentiments = []\r\n",
        "MegaDF = []\r\n",
        "all_words = []\r\n",
        "\r\n",
        "#get list of all words\r\n",
        "#first true\r\n",
        "#sentiments are also arranged i.e true = 1, false = 0\r\n",
        "for i in range(len(true_df)):\r\n",
        "  blob = TextBlob(true_df.iloc[i][0])\r\n",
        "  sentiments.append(1)\r\n",
        "  for word in blob.words:\r\n",
        "    all_words.append(word)\r\n",
        "#then fake added\r\n",
        "for i in range(len(fake_df)):\r\n",
        "  blob = TextBlob(fake_df.iloc[i][0])\r\n",
        "  sentiments.append(0)\r\n",
        "  for word in blob.words:\r\n",
        "    all_words.append(word)\r\n",
        "\r\n",
        "#make unique\r\n",
        "unique_words = set(all_words)\r\n",
        "print(len(unique_words))\r\n",
        "\r\n",
        "#make mega list containing both true and fake\r\n",
        "\r\n",
        "for i in range(len(true_df)):\r\n",
        "  MegaDF.append(true_df.iloc[i][0])\r\n",
        "\r\n",
        "for i in range(len(fake_df)):\r\n",
        "  MegaDF.append(fake_df.iloc[i][0])\r\n",
        "\r\n",
        "#one_hot converts titles to list of word-numbers\r\n",
        "embedded_sentences = [one_hot(sent, 40000) for sent in MegaDF]\r\n",
        "print(embedded_sentences[0])\r\n",
        "\r\n",
        "#lambda is basically a short function\r\n",
        "#want to find max number of word numbers in a title\r\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\r\n",
        "longest_sentence = max(MegaDF, key=word_count)\r\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))\r\n",
        "\r\n",
        "#the smaller ones are padded with zeroes at the end\r\n",
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "MLModel = keras.Sequential()\r\n",
        "#MLModel.layers()\r\n",
        "\"\"\"model.add(Dense(12, input_dim=len(TrueDF), activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\"\"\"\r\n",
        "\r\n",
        "#40k is vocabulary. checked by printing unique word count\r\n",
        "#60 is dimensions of word vector output\r\n",
        "MLModel.add(keras.layers.Embedding(40000, 60, input_length=length_long_sentence))\r\n",
        "\r\n",
        "#dk what this do\r\n",
        "MLModel.add(keras.layers.Flatten())\r\n",
        "MLModel.add(keras.layers.Dense(1, activation='sigmoid'))\r\n",
        "sentiments_array = array(sentiments)\r\n",
        "MLModel.summary()\r\n",
        "\r\n",
        "MLModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "MLModel.fit(padded_sentences, sentiments_array, epochs=6)\r\n",
        "\r\n",
        "#test data\r\n",
        "fake_test = pd.read_csv('fake.csv')\r\n",
        "\r\n",
        "#remove rows where title is NaN\r\n",
        "fake_test.dropna(subset=['title'], inplace=True)\r\n",
        "\r\n",
        "#turn to list\r\n",
        "fake_list = fake_test['title'].tolist()\r\n",
        "\r\n",
        "#test data is all fake\r\n",
        "y = [0]*len(fake_list)\r\n",
        "\r\n",
        "#turn to numpy array list no good\r\n",
        "y_array = array(y)\r\n",
        "\r\n",
        "#one_hot converts sentence to list of word-numbers\r\n",
        "embedded_sentences2 = [one_hot(sent, 40000) for sent in fake_list]\r\n",
        "\r\n",
        "#pad with zeroes to max len\r\n",
        "padded_sentences2 = pad_sequences(embedded_sentences2, length_long_sentence, padding='post')\r\n",
        "\r\n",
        "_, accuracy = MLModel.evaluate(padded_sentences2, y_array, verbose=0)\r\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4ccc8a935472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#then fake added\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0msentiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1479\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1482\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mis_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     ):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsEEtO_7YIRx"
      },
      "source": [
        "# V2 -> Feed forward\r\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWLXx7_X42yY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bd520e-fca3-49eb-ecdb-f288cf7fad66"
      },
      "source": [
        "# Sources:\r\n",
        "# https://stackoverflow.com/questions/34714070/error-tokenizing-data-c-error-eof-following-escape-character\r\n",
        "# https://stackoverflow.com/questions/26147180/convert-row-to-column-header-for-pandas-dataframe\r\n",
        "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\r\n",
        "# https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\r\n",
        "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\r\n",
        "# https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\r\n",
        "# https://www.youtube.com/watch?v=bKLL0tAj3GE&ab_channel=BhaveshBhatt\r\n",
        "# https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python\r\n",
        "# programiz.com/python-programming/json\r\n",
        "# https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn\r\n",
        "\r\n",
        "# Focus mainly on headlines\r\n",
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Set up data frames\r\n",
        "true_df = pd.read_csv('True.csv',  header=None)\r\n",
        "true_df.rename(columns=true_df.iloc[0],inplace=True)\r\n",
        "true_df = true_df.drop(true_df.index[0])\r\n",
        "TrueCol = [1 for i in range(true_df.shape[0])]\r\n",
        "true_df[\"Label\"] = TrueCol\r\n",
        "\r\n",
        "fake_df = pd.read_csv('Fake.csv',  header=None)\r\n",
        "fake_df.rename(columns=fake_df.iloc[0],inplace=True)\r\n",
        "fake_df= fake_df.drop(fake_df.index[0])\r\n",
        "FakeCol = [0 for i in range(fake_df.shape[0])]\r\n",
        "fake_df[\"Label\"] = FakeCol\r\n",
        "\r\n",
        "# Prepare Training Set\r\n",
        "TrainingSet  = pd.concat([fake_df,true_df])\r\n",
        "TrainingSet  = TrainingSet.sample(frac=1).reset_index(drop=True)\r\n",
        "TrainingSet = TrainingSet.drop(axis=1,columns=[\"subject\",\"date\",\"text\"])\r\n",
        "TrainingSet = TrainingSet.loc[:10000,:]\r\n",
        "\r\n",
        "# Vectorize with bag of words model\r\n",
        "Vectorizer = TfidfVectorizer(stop_words=\"english\")\r\n",
        "TokenizedText = Vectorizer.fit_transform(TrainingSet[\"title\"].to_list()).toarray().tolist()\r\n",
        "# Setup Model\r\n",
        "Model = Sequential()\r\n",
        "Model.add(Dense(80, input_dim=len(TokenizedText[0]), activation='relu'))\r\n",
        "Model.add(Dense(40, activation='relu'))\r\n",
        "Model.add(Dense(40, activation='relu'))\r\n",
        "Model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# Compile and train\r\n",
        "Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "XTrain, XTest, YTrain, YTest = train_test_split(TokenizedText,TrainingSet[\"Label\"].tolist(), test_size=0.3, random_state=21)\r\n",
        "Model.fit(XTrain,YTrain,batch_size=100,epochs=20)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "70/70 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.7295\n",
            "Epoch 2/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.1406 - accuracy: 0.9642\n",
            "Epoch 3/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.9954\n",
            "Epoch 4/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9993\n",
            "Epoch 5/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 7.4592e-04 - accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 5.3412e-04 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 3.9937e-04 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 3.0572e-04 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 2.3479e-04 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 2.0269e-04 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 1.6095e-04 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 1.3252e-04 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 1.1052e-04 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 9.3809e-05 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 8.2562e-05 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 7.0750e-05 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 6.6279e-05 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "70/70 [==============================] - 0s 4ms/step - loss: 5.9274e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5474ca5860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVlTwKkwGyjW"
      },
      "source": [
        "# Get predictions\r\n",
        "PredictionResults = Model.predict(XTest)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uj9N0XtTARb"
      },
      "source": [
        "# Get predication results(normalized)\r\n",
        "TestArray = []\r\n",
        "for i in PredictionResults:\r\n",
        "  if i >= 0.5:\r\n",
        "    TestArray.append(1)\r\n",
        "  else:\r\n",
        "    TestArray.append(0)\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7pQ3pBJTfUl",
        "outputId": "9e2100b1-c1ed-44d3-f915-e33197596f76"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "# Good in da hood\r\n",
        "print(\"Accurracy: ~{}\".format(round(accuracy_score(YTest,TestArray)*100)))\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurracy: ~91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Uxo6VNeYhM4",
        "outputId": "e58ef742-e784-4917-b890-9acfb6d5a6eb"
      },
      "source": [
        "# Convert to Tensorflow lite for website purposes\r\n",
        "from tensorflow import lite\r\n",
        "Converter = lite.TFLiteConverter.from_keras_model(Model)\r\n",
        "TFLiteModel = Converter.convert()\r\n",
        "open(\"MainMode.tflite\",\"wb\").write(TFLiteModel)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp4_l43j34/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3928124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8gnHE5VaOps"
      },
      "source": [
        "# Test TFLite model (thing that will go to the website)\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "interpreter = tf.lite.Interpreter(model_path=\"MainMode.tflite\")\r\n",
        "interpreter.allocate_tensors()\r\n",
        "\r\n",
        "input_details = interpreter.get_input_details()\r\n",
        "output_details = interpreter.get_output_details()\r\n",
        "# Test model on random input data.\r\n",
        "input_shape = input_details[0]['shape']\r\n",
        "\r\n",
        "for i in range(len(XTest)):\r\n",
        "  input_data = np.array(XTest[i:i+1], dtype=np.float32)\r\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
        "\r\n",
        "  interpreter.invoke()\r\n",
        "\r\n",
        "  # The function `get_tensor()` returns a copy of the tensor data.\r\n",
        "  # Use `tensor()` in order to get a pointer to the tensor.\r\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
        "  Result = output_data[0][0]\r\n",
        "  if Result >= 0.5:\r\n",
        "    print(1)\r\n",
        "  else:\r\n",
        "    print(0)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjaBd9CPXu03"
      },
      "source": [
        "\r\n",
        "pickle.dump(Vectorizer,open(\"Vectorizer.pickle\",\"wb\"))"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}